{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from supervision.assets import download_assets, VideoAssets\n",
    "\n",
    "# download_assets(VideoAssets.VEHICLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "from ultralytics.solutions import speed_estimation\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(\"yolov8m.yaml\")\n",
    "names = model.model.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(\"vehicles.mp4\")\n",
    "assert cap.isOpened()\n",
    "w,h,fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\n",
    "video_writer = cv2.VideoWriter(\"speed_estimation.avi\",cv2.VideoWriter_fourcc(*'mp4v'),\n",
    "                               fps,\n",
    "                               (w,h))\n",
    "line_pts = [(0,h//2),(w,h//2)]\n",
    "speed_obj = speed_estimation.SpeedEstimator()\n",
    "speed_obj.set_args(reg_pts=line_pts,\n",
    "                   names=names,\n",
    "                   view_img=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while cap.isOpened():\n",
    "    success, im0 = cap.read()\n",
    "    if not success:\n",
    "        print(\"video frame is empty or not being read properly\")\n",
    "        break\n",
    "    tracks = model.track(im0, persist=True, show = False)\n",
    "    im0 = speed_obj.estimate_speed(im0, tracks)\n",
    "    video_writer.write(im0)\n",
    "cap.release()\n",
    "video_writer.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 cars, 1 truck, 99.6ms\n",
      "Speed: 5.5ms preprocess, 99.6ms inference, 147.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 1 truck, 15.6ms\n",
      "Speed: 8.0ms preprocess, 15.6ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 1 truck, 16.2ms\n",
      "Speed: 5.0ms preprocess, 16.2ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 1 truck, 13.8ms\n",
      "Speed: 4.4ms preprocess, 13.8ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 1 truck, 21.0ms\n",
      "Speed: 5.0ms preprocess, 21.0ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 1 bus, 23.0ms\n",
      "Speed: 4.5ms preprocess, 23.0ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 1 truck, 17.0ms\n",
      "Speed: 4.5ms preprocess, 17.0ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 1 truck, 10.5ms\n",
      "Speed: 3.4ms preprocess, 10.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 1 truck, 22.0ms\n",
      "Speed: 3.0ms preprocess, 22.0ms inference, 6.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 1 truck, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 1 truck, 11.0ms\n",
      "Speed: 2.0ms preprocess, 11.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 1 bus, 11.0ms\n",
      "Speed: 3.0ms preprocess, 11.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 1 bus, 10.5ms\n",
      "Speed: 2.5ms preprocess, 10.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 1 bus, 11.6ms\n",
      "Speed: 3.0ms preprocess, 11.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 cars, 1 bus, 11.7ms\n",
      "Speed: 2.0ms preprocess, 11.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 cars, 1 truck, 11.0ms\n",
      "Speed: 2.0ms preprocess, 11.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 cars, 1 truck, 9.5ms\n",
      "Speed: 3.3ms preprocess, 9.5ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 cars, 1 truck, 20.8ms\n",
      "Speed: 6.4ms preprocess, 20.8ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 cars, 1 truck, 14.5ms\n",
      "Speed: 4.6ms preprocess, 14.5ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 1 truck, 15.5ms\n",
      "Speed: 5.1ms preprocess, 15.5ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 1 truck, 24.9ms\n",
      "Speed: 4.5ms preprocess, 24.9ms inference, 6.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 1 truck, 25.3ms\n",
      "Speed: 4.5ms preprocess, 25.3ms inference, 5.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 1 truck, 24.0ms\n",
      "Speed: 4.9ms preprocess, 24.0ms inference, 5.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 1 truck, 24.4ms\n",
      "Speed: 4.7ms preprocess, 24.4ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 1 truck, 24.6ms\n",
      "Speed: 7.1ms preprocess, 24.6ms inference, 4.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 1 truck, 24.8ms\n",
      "Speed: 4.0ms preprocess, 24.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 1 bus, 25.4ms\n",
      "Speed: 3.0ms preprocess, 25.4ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 1 truck, 24.0ms\n",
      "Speed: 4.7ms preprocess, 24.0ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 1 bus, 1 truck, 28.5ms\n",
      "Speed: 5.0ms preprocess, 28.5ms inference, 7.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 1 bus, 1 truck, 24.8ms\n",
      "Speed: 5.9ms preprocess, 24.8ms inference, 5.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 1 bus, 1 truck, 25.4ms\n",
      "Speed: 4.5ms preprocess, 25.4ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 cars, 1 bus, 1 truck, 24.2ms\n",
      "Speed: 5.2ms preprocess, 24.2ms inference, 5.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 cars, 24.8ms\n",
      "Speed: 5.0ms preprocess, 24.8ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 cars, 1 truck, 25.0ms\n",
      "Speed: 4.6ms preprocess, 25.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 cars, 1 truck, 25.1ms\n",
      "Speed: 4.6ms preprocess, 25.1ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 cars, 1 truck, 24.4ms\n",
      "Speed: 4.8ms preprocess, 24.4ms inference, 5.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 cars, 1 truck, 24.4ms\n",
      "Speed: 4.5ms preprocess, 24.4ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 cars, 2 trucks, 25.6ms\n",
      "Speed: 4.2ms preprocess, 25.6ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 cars, 1 bus, 25.1ms\n",
      "Speed: 4.6ms preprocess, 25.1ms inference, 4.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 cars, 1 bus, 25.2ms\n",
      "Speed: 5.4ms preprocess, 25.2ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 cars, 1 truck, 24.2ms\n",
      "Speed: 5.2ms preprocess, 24.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 cars, 1 truck, 28.8ms\n",
      "Speed: 4.8ms preprocess, 28.8ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 cars, 1 truck, 24.8ms\n",
      "Speed: 5.5ms preprocess, 24.8ms inference, 5.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 cars, 1 truck, 26.5ms\n",
      "Speed: 7.1ms preprocess, 26.5ms inference, 5.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 cars, 1 truck, 24.6ms\n",
      "Speed: 3.8ms preprocess, 24.6ms inference, 4.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 cars, 2 trucks, 24.7ms\n",
      "Speed: 4.0ms preprocess, 24.7ms inference, 4.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 cars, 1 truck, 24.7ms\n",
      "Speed: 4.7ms preprocess, 24.7ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 cars, 2 trucks, 24.4ms\n",
      "Speed: 5.1ms preprocess, 24.4ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 cars, 2 trucks, 25.1ms\n",
      "Speed: 4.0ms preprocess, 25.1ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 cars, 2 trucks, 26.1ms\n",
      "Speed: 3.0ms preprocess, 26.1ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 cars, 2 trucks, 26.0ms\n",
      "Speed: 5.8ms preprocess, 26.0ms inference, 5.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 cars, 1 truck, 24.5ms\n",
      "Speed: 4.5ms preprocess, 24.5ms inference, 4.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 cars, 2 trucks, 24.5ms\n",
      "Speed: 5.0ms preprocess, 24.5ms inference, 5.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 cars, 2 trucks, 26.1ms\n",
      "Speed: 5.0ms preprocess, 26.1ms inference, 7.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 cars, 1 truck, 26.5ms\n",
      "Speed: 4.0ms preprocess, 26.5ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 2 trucks, 25.0ms\n",
      "Speed: 5.0ms preprocess, 25.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 2 trucks, 25.6ms\n",
      "Speed: 4.5ms preprocess, 25.6ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 cars, 1 truck, 25.0ms\n",
      "Speed: 4.4ms preprocess, 25.0ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 cars, 1 truck, 25.1ms\n",
      "Speed: 3.0ms preprocess, 25.1ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 cars, 1 truck, 24.2ms\n",
      "Speed: 4.0ms preprocess, 24.2ms inference, 5.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 cars, 1 truck, 25.0ms\n",
      "Speed: 4.6ms preprocess, 25.0ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 2 trucks, 24.6ms\n",
      "Speed: 5.0ms preprocess, 24.6ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 cars, 1 truck, 24.7ms\n",
      "Speed: 6.5ms preprocess, 24.7ms inference, 7.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 1 truck, 25.6ms\n",
      "Speed: 4.0ms preprocess, 25.6ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 1 truck, 23.9ms\n",
      "Speed: 4.0ms preprocess, 23.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 1 truck, 25.2ms\n",
      "Speed: 4.6ms preprocess, 25.2ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 1 truck, 25.7ms\n",
      "Speed: 4.8ms preprocess, 25.7ms inference, 4.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 1 truck, 25.6ms\n",
      "Speed: 5.0ms preprocess, 25.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 1 truck, 25.0ms\n",
      "Speed: 8.5ms preprocess, 25.0ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 1 truck, 25.5ms\n",
      "Speed: 4.5ms preprocess, 25.5ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 1 truck, 25.0ms\n",
      "Speed: 4.7ms preprocess, 25.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 1 truck, 24.7ms\n",
      "Speed: 3.0ms preprocess, 24.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 1 truck, 25.6ms\n",
      "Speed: 6.7ms preprocess, 25.6ms inference, 4.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 1 truck, 23.7ms\n",
      "Speed: 6.4ms preprocess, 23.7ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 1 truck, 24.3ms\n",
      "Speed: 7.5ms preprocess, 24.3ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 1 truck, 24.9ms\n",
      "Speed: 6.9ms preprocess, 24.9ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 1 truck, 24.4ms\n",
      "Speed: 7.2ms preprocess, 24.4ms inference, 8.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 1 bench, 24.4ms\n",
      "Speed: 4.0ms preprocess, 24.4ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 1 truck, 23.9ms\n",
      "Speed: 5.5ms preprocess, 23.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 1 truck, 27.4ms\n",
      "Speed: 7.5ms preprocess, 27.4ms inference, 7.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 car, 1 truck, 24.1ms\n",
      "Speed: 6.0ms preprocess, 24.1ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 24.4ms\n",
      "Speed: 5.0ms preprocess, 24.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 27.6ms\n",
      "Speed: 7.1ms preprocess, 27.6ms inference, 4.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 25.2ms\n",
      "Speed: 4.0ms preprocess, 25.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 26.9ms\n",
      "Speed: 7.5ms preprocess, 26.9ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 24.5ms\n",
      "Speed: 4.5ms preprocess, 24.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 24.8ms\n",
      "Speed: 7.0ms preprocess, 24.8ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 25.1ms\n",
      "Speed: 4.0ms preprocess, 25.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 24.7ms\n",
      "Speed: 8.1ms preprocess, 24.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 25.1ms\n",
      "Speed: 4.0ms preprocess, 25.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 24.6ms\n",
      "Speed: 7.5ms preprocess, 24.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 24.5ms\n",
      "Speed: 6.0ms preprocess, 24.5ms inference, 5.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 24.8ms\n",
      "Speed: 4.0ms preprocess, 24.8ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 25.0ms\n",
      "Speed: 4.5ms preprocess, 25.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 25.5ms\n",
      "Speed: 5.5ms preprocess, 25.5ms inference, 5.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 23.9ms\n",
      "Speed: 5.3ms preprocess, 23.9ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 24.8ms\n",
      "Speed: 5.0ms preprocess, 24.8ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 24.6ms\n",
      "Speed: 9.2ms preprocess, 24.6ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 24.4ms\n",
      "Speed: 5.5ms preprocess, 24.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 24.5ms\n",
      "Speed: 7.5ms preprocess, 24.5ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 23.7ms\n",
      "Speed: 4.0ms preprocess, 23.7ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 23.9ms\n",
      "Speed: 7.0ms preprocess, 23.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 24.6ms\n",
      "Speed: 5.0ms preprocess, 24.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 24.2ms\n",
      "Speed: 4.5ms preprocess, 24.2ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 25.4ms\n",
      "Speed: 5.2ms preprocess, 25.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 24.3ms\n",
      "Speed: 4.0ms preprocess, 24.3ms inference, 4.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 24.2ms\n",
      "Speed: 4.0ms preprocess, 24.2ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 24.9ms\n",
      "Speed: 3.5ms preprocess, 24.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 24.9ms\n",
      "Speed: 5.1ms preprocess, 24.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 42\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVideo frame is empty or video processing has been \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;124m    successfully completed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m tracks \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpersist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m im0 \u001b[38;5;241m=\u001b[39m speed_obj\u001b[38;5;241m.\u001b[39mestimate_speed(im0, tracks)\n\u001b[0;32m     45\u001b[0m video_writer\u001b[38;5;241m.\u001b[39mwrite(im0)\n",
      "File \u001b[1;32ma:\\anaconda3\\envs\\yolov8\\lib\\site-packages\\ultralytics\\engine\\model.py:478\u001b[0m, in \u001b[0;36mModel.track\u001b[1;34m(self, source, stream, persist, **kwargs)\u001b[0m\n\u001b[0;32m    476\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconf\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconf\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0.1\u001b[39m  \u001b[38;5;66;03m# ByteTrack-based method needs low confidence predictions as input\u001b[39;00m\n\u001b[0;32m    477\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrack\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 478\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(source\u001b[38;5;241m=\u001b[39msource, stream\u001b[38;5;241m=\u001b[39mstream, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32ma:\\anaconda3\\envs\\yolov8\\lib\\site-packages\\ultralytics\\engine\\model.py:439\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[0;32m    437\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_prompts\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[0;32m    438\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mset_prompts(prompts)\n\u001b[1;32m--> 439\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict_cli(source\u001b[38;5;241m=\u001b[39msource) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32ma:\\anaconda3\\envs\\yolov8\\lib\\site-packages\\ultralytics\\engine\\predictor.py:206\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[1;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32ma:\\anaconda3\\envs\\yolov8\\lib\\site-packages\\torch\\utils\\_contextlib.py:35\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m---> 35\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     38\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     39\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[1;32ma:\\anaconda3\\envs\\yolov8\\lib\\site-packages\\ultralytics\\engine\\predictor.py:294\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[1;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m2\u001b[39m]:\n\u001b[0;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(preds, im, im0s)\n\u001b[1;32m--> 294\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_callbacks\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mon_predict_postprocess_end\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;66;03m# Visualize, save, write results\u001b[39;00m\n\u001b[0;32m    296\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(im0s)\n",
      "File \u001b[1;32ma:\\anaconda3\\envs\\yolov8\\lib\\site-packages\\ultralytics\\engine\\predictor.py:403\u001b[0m, in \u001b[0;36mBasePredictor.run_callbacks\u001b[1;34m(self, event)\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Runs all registered callbacks for a specific event.\"\"\"\u001b[39;00m\n\u001b[0;32m    402\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mget(event, []):\n\u001b[1;32m--> 403\u001b[0m     \u001b[43mcallback\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32ma:\\anaconda3\\envs\\yolov8\\lib\\site-packages\\ultralytics\\trackers\\track.py:63\u001b[0m, in \u001b[0;36mon_predict_postprocess_end\u001b[1;34m(predictor, persist)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(det) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m---> 63\u001b[0m tracks \u001b[38;5;241m=\u001b[39m \u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrackers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mim0s\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tracks) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[1;32ma:\\anaconda3\\envs\\yolov8\\lib\\site-packages\\ultralytics\\trackers\\byte_tracker.py:293\u001b[0m, in \u001b[0;36mBYTETracker.update\u001b[1;34m(self, results, img)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmulti_predict(strack_pool)\n\u001b[0;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgmc\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m img \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 293\u001b[0m     warp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgmc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    294\u001b[0m     STrack\u001b[38;5;241m.\u001b[39mmulti_gmc(strack_pool, warp)\n\u001b[0;32m    295\u001b[0m     STrack\u001b[38;5;241m.\u001b[39mmulti_gmc(unconfirmed, warp)\n",
      "File \u001b[1;32ma:\\anaconda3\\envs\\yolov8\\lib\\site-packages\\ultralytics\\trackers\\utils\\gmc.py:102\u001b[0m, in \u001b[0;36mGMC.apply\u001b[1;34m(self, raw_frame, detections)\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapplyEcc(raw_frame)\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmethod \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparseOptFlow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapplySparseOptFlow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_frame\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39meye(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n",
      "File \u001b[1;32ma:\\anaconda3\\envs\\yolov8\\lib\\site-packages\\ultralytics\\trackers\\utils\\gmc.py:311\u001b[0m, in \u001b[0;36mGMC.applySparseOptFlow\u001b[1;34m(self, raw_frame)\u001b[0m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    296\u001b[0m \u001b[38;5;124;03mApply Sparse Optical Flow method to a raw frame.\u001b[39;00m\n\u001b[0;32m    297\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;124;03m           [4, 5, 6]])\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    310\u001b[0m height, width, _ \u001b[38;5;241m=\u001b[39m raw_frame\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m--> 311\u001b[0m frame \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcvtColor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_frame\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOLOR_BGR2GRAY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    312\u001b[0m H \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39meye(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m    314\u001b[0m \u001b[38;5;66;03m# Downscale image\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "from ultralytics.solutions import speed_estimation\n",
    "import cv2\n",
    "import torch\n",
    "\n",
    "# torch.cuda.is_available = lambda : False\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "device = \"0\" if torch.cuda.is_available() else \"cpu\"\n",
    "if device == \"0\":\n",
    "    torch.cuda.set_device(0)\n",
    "model = YOLO(\"yolov8n.pt\")\n",
    "names = model.model.names\n",
    "\n",
    "cap = cv2.VideoCapture(\"vehicles.mp4\")\n",
    "assert cap.isOpened(), \"Error reading video file\"\n",
    "w, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, \n",
    "cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\n",
    "\n",
    "# Video writer\n",
    "video_writer = cv2.VideoWriter(\"speed_estimation.avi\",\n",
    "                               cv2.VideoWriter_fourcc(*'mp4v'),\n",
    "                               fps,\n",
    "                               (w, h))\n",
    "\n",
    "line_pts = [(0, h//2), (w,h//2)]\n",
    "\n",
    "# Init speed-estimation obj\n",
    "speed_obj = speed_estimation.SpeedEstimator()\n",
    "speed_obj.set_args(reg_pts=line_pts,\n",
    "                   names=names,\n",
    "                   view_img=True)\n",
    "\n",
    "while cap.isOpened():\n",
    "\n",
    "    success, im0 = cap.read()\n",
    "    if not success:\n",
    "        print(\"Video frame is empty or video processing has been \\\n",
    "        successfully completed.\")\n",
    "        break\n",
    "\n",
    "    tracks = model.track(im0, persist=True, show=False)\n",
    "\n",
    "    im0 = speed_obj.estimate_speed(im0, tracks)\n",
    "    video_writer.write(im0)\n",
    "\n",
    "cap.release()\n",
    "video_writer.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "from ultralytics.solutions import heatmap\n",
    "import cv2\n",
    "import torch\n",
    "\n",
    "device = \"0\" if torch.cuda.is_available() else \"cpu\"\n",
    "if device == \"0\":\n",
    "    torch.cuda.set_device(0)\n",
    "    \n",
    "model = YOLO(\"yolov8n.pt\")\n",
    "cap = cv2.VideoCapture(\"vehicles.mp4\")\n",
    "assert cap.isOpened(), \"Error reading video file\"\n",
    "w, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, \n",
    "cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\n",
    "\n",
    "# Video writer\n",
    "video_writer = cv2.VideoWriter(\"heatmap_output.avi\",\n",
    "                               cv2.VideoWriter_fourcc(*'mp4v'),\n",
    "                               fps,\n",
    "                               (320,640))\n",
    "\n",
    "# Init heatmap\n",
    "heatmap_obj = heatmap.Heatmap()\n",
    "heatmap_obj.set_args(colormap=cv2.COLORMAP_PARULA,\n",
    "                     imw=w,\n",
    "                     imh=h,\n",
    "                     view_img=True,\n",
    "                     shape=\"circle\")\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, im0 = cap.read()\n",
    "    if not success:\n",
    "        print(\"Video frame is empty or video processing has \\\n",
    "                been successfully completed.\")\n",
    "        break\n",
    "    tracks = model.track(im0, persist=True, show=False)\n",
    "\n",
    "    im0 = heatmap_obj.generate_heatmap(im0, tracks)\n",
    "    video_writer.write(im0)\n",
    "\n",
    "cap.release()\n",
    "video_writer.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
